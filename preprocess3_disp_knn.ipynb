{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Worked on by: Meena Hari and Tarini Singh.\n",
    "\n",
    "We perform data preprocessing using KNearestNeighbors.\n",
    "66 new features are generated.\n",
    "\n",
    "Trained a 1 layer ANN with transformed, higher dimensional \n",
    "dataset (each input consists of the raw board representaion \n",
    "(list of integers from 1 - 16) plus 66 newly generated features).\n",
    "\n",
    "In prog.\n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from tqdm import tqdm\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Conv2D, Flatten, Input\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import load_model\n",
    "import keras.losses\n",
    "\n",
    "from constants import *\n",
    "import heuristic as h\n",
    "import io_help as io\n",
    "import neural_net as nn\n",
    "import solver as s\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    This function reads in training data from a file and returns \n",
    "    the boards in X and their labels in Y as a tuple. \n",
    "    \"\"\"\n",
    "    file = open(file_name, \"r\")\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "\n",
    "    for string in file: \n",
    "        (board, dist) = io.string_to_board_and_dist(string)\n",
    "        X_temp = np.concatenate((board.reshape(16)), axis=None)\n",
    "        X.append(X_temp)\n",
    "        Y.append(dist)\n",
    "        \n",
    "    file.close()\n",
    "    X_train = np.asarray(X)\n",
    "    Y_train = np.asarray(Y)\n",
    "    return(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(93844, 16)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset. \n",
    "# X: board inputs, Y: true output.\n",
    "(X_train,Y_train) = load_data('Uncombined Data Files/meena_5_19_2020_93844.txt')\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates additional features.\n",
    "# X: the input data file.\n",
    "# X_train: the original training data file (not transformed).\n",
    "def gen_features (X, X_train, knn_model):\n",
    "    #data_arr = np.zeros([len(X), 16*2*2 + 2])\n",
    "    data_arr = np.zeros([len(X), (16)*2*2 + 2])\n",
    "   # disp_2D = np.zeros([len(X), 32])\n",
    "    man_ham_2D = np.zeros([len(X), 2])\n",
    "    one_hot_2D = np.zeros([len(X), 256+32])\n",
    "    pred = knn_model.kneighbors(X)\n",
    "    \n",
    "    \n",
    "    #for i in tqdm(range(len(X))):\n",
    "    for i in range(len(X)):\n",
    "        row = X[i]\n",
    "        # Grabs the rows in X corresponding to 50 nearest neighbors of X[i].\n",
    "        # pred[1][i] contains a list of the indices of the 50 nearest neighbors.\n",
    "        data = X_train[pred[1][i]]\n",
    "        # Divide X[i] by each of its neighbors. div should be a \n",
    "        # 50 x 16 matrix, i.e. div[j] = X[i] / X[j].\n",
    "        div = (row / data)\n",
    "        # Subtract X[i] by each of its neighbors. diff should be a \n",
    "        # 50 x 16 dimension matrix.\n",
    "        diff = (row - data)\n",
    "        # concat is a 50 x 32 matrix.\n",
    "        concat = np.concatenate([div, diff], axis = 1)\n",
    "        # means is a 50 x 32 matrix.\n",
    "        # std is a 50 x 32 matrix.\n",
    "        means, stds = np.nanmean(concat, axis = 0), np.nanstd(concat, axis = 0)\n",
    "        # Populate data_arr with newly generated features.\n",
    "        data_arr[i, :len(means)] = means\n",
    "        data_arr[i, len(means):len(means) + len(stds)] = stds\n",
    "        data_arr[i, -1] = np.nanmean(pred[0][i])\n",
    "        data_arr[i, -2] = np.nanstd(pred[0][i])\n",
    "        \n",
    "        # Calculate Displacements\n",
    "       # disp_2D[i] = nn.calc_displacements(row.reshape(4,4))\n",
    "        \n",
    "        # Manhattan, Hamming distances\n",
    "        man = h.manhattan(row.reshape(4,4), None)\n",
    "        ham = h.hamming(row.reshape(4,4), None)\n",
    "        man_ham_2D[i,0] = man\n",
    "        man_ham_2D[i,1] = ham\n",
    "        one_hot_2D[i] = nn.get_rep_2(row.reshape(4,4))\n",
    "        \n",
    "    # Concatenate generated features to the original dataset.\n",
    "    return np.concatenate([data_arr, one_hot_2D, man_ham_2D], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 93844/93844 [00:59<00:00, 1564.66it/s]\n"
     ]
    }
   ],
   "source": [
    "knn_model = NearestNeighbors(n_neighbors=151, n_jobs = -1).fit(X_train,Y_train)\n",
    "X_train_2 = gen_features(X_train, X_train, knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93844, 356)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_mse(y_true, y_pred):\n",
    "    \"\"\"custom loss functions\"\"\"\n",
    "    loss = (1 + 6/(1 + K.exp(-(y_pred - y_true)))) * K.square(y_pred - y_true)\n",
    "    loss = K.mean(loss, axis = 1)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def exp_loss_2(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function. \n",
    "    \"\"\"\n",
    "    loss = K.exp((y_pred - y_true))\n",
    "    loss = loss + K.square(y_pred - y_true)\n",
    "    loss = K.mean(loss, axis = 1)\n",
    "\n",
    "    return loss\n",
    "    \n",
    "keras.losses.shift_mse = shift_mse\n",
    "keras.losses.exp_loss_2 = exp_loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def luka_model (X, Y):\n",
    "    # Build Model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input Layer\n",
    "    i = Input(shape = (16*2*2+2+(256 + 32 + 2),))\n",
    "    x_1 = Dense(16*2*2+2+(256 + 32 + 2), activation='relu')(i)\n",
    "    x_2 = Dropout(0.1)(x_1)\n",
    "    x_3 = Dense(356, activation='relu')(x_2)\n",
    "    x_4 = Dropout(0.1)(x_3)\n",
    "    x_5 = Dense(17, activation='relu')(x_4)\n",
    "    o = Dense(1, activation='linear')(x_1)\n",
    "    model = Model(i,o)\n",
    "\n",
    "    # Define the optimizer and loss function\n",
    "    model.compile(optimizer='adam', loss=exp_loss_2, metrics=['accuracy'])\n",
    "\n",
    "    # You can also define a custom loss function\n",
    "    # model.compile(optimizer='adam', loss=custom_loss)\n",
    "\n",
    "    # Train \n",
    "    model.fit(X, Y, epochs=15)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Build Model\n",
    "model = Sequential()\n",
    "\n",
    "# Input Layer\n",
    "model.add(Dense(units=(16*2*2+2+16), input_dim=(16*2*2+2+16), activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# Hidden Layers\n",
    "model.add(Dense(units=66+16, activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "# Define the optimizer and loss function\n",
    "#model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "model.compile(optimizer='adam', loss=shift_mse, metrics=['accuracy'])\n",
    "\n",
    "# You can also define a custom loss function\n",
    "# model.compile(optimizer='adam', loss=custom_loss)\n",
    "\n",
    "# Train \n",
    "model.fit(X_train_2, Y_train, epochs=20)\n",
    "\n",
    "# Test\n",
    "#score = model.evaluate(X_test, Y_test)\n",
    "\n",
    "#print(score)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10047/10047 [00:07<00:00, 1291.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset. \n",
    "# X_test: board inputs, Y_test: true output.\n",
    "(X_test,Y_test) = load_data('Uncombined Data Files/Yasmin_5_19_10048.txt')\n",
    "\n",
    "# Transform X_test to higher dimension.\n",
    "X_test_2 = gen_features (X_test, X_train, knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "10047/10047 [==============================] - 1s 59us/step - loss: 17.3825 - accuracy: 0.2549\n",
      "Epoch 2/15\n",
      "10047/10047 [==============================] - 1s 69us/step - loss: 7.4225 - accuracy: 0.3353\n",
      "Epoch 3/15\n",
      "10047/10047 [==============================] - 1s 50us/step - loss: 6.1292 - accuracy: 0.3509\n",
      "Epoch 4/15\n",
      "10047/10047 [==============================] - 1s 65us/step - loss: 4.9936 - accuracy: 0.3553\n",
      "Epoch 5/15\n",
      "10047/10047 [==============================] - 1s 68us/step - loss: 4.2447 - accuracy: 0.3619 0s - loss: 4\n",
      "Epoch 6/15\n",
      "10047/10047 [==============================] - 0s 49us/step - loss: 3.4058 - accuracy: 0.4034\n",
      "Epoch 7/15\n",
      "10047/10047 [==============================] - 1s 50us/step - loss: 2.8548 - accuracy: 0.4212\n",
      "Epoch 8/15\n",
      "10047/10047 [==============================] - 0s 48us/step - loss: 2.4249 - accuracy: 0.4464\n",
      "Epoch 9/15\n",
      "10047/10047 [==============================] - 0s 49us/step - loss: 2.0823 - accuracy: 0.4711\n",
      "Epoch 10/15\n",
      "10047/10047 [==============================] - 0s 49us/step - loss: 1.8576 - accuracy: 0.4881\n",
      "Epoch 11/15\n",
      "10047/10047 [==============================] - 0s 50us/step - loss: 1.6945 - accuracy: 0.5010\n",
      "Epoch 12/15\n",
      "10047/10047 [==============================] - 0s 48us/step - loss: 1.5925 - accuracy: 0.5083\n",
      "Epoch 13/15\n",
      "10047/10047 [==============================] - 1s 50us/step - loss: 1.4827 - accuracy: 0.5284\n",
      "Epoch 14/15\n",
      "10047/10047 [==============================] - 0s 49us/step - loss: 1.4347 - accuracy: 0.5500 0s - loss: 1.4066 - ac\n",
      "Epoch 15/15\n",
      "10047/10047 [==============================] - 1s 52us/step - loss: 1.3232 - accuracy: 0.5654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10047 [00:00<?, ?it/s]\u001b[A\n",
      "  1%|          | 101/10047 [00:00<00:09, 1008.02it/s]\u001b[A\n",
      "  2%|▏         | 220/10047 [00:00<00:09, 1054.94it/s]\u001b[A\n",
      "  3%|▎         | 345/10047 [00:00<00:08, 1105.06it/s]\u001b[A\n",
      "  5%|▍         | 473/10047 [00:00<00:08, 1150.62it/s]\u001b[A\n",
      "  6%|▌         | 590/10047 [00:00<00:08, 1154.73it/s]\u001b[A\n",
      "  7%|▋         | 704/10047 [00:00<00:08, 1149.64it/s]\u001b[A\n",
      "  8%|▊         | 814/10047 [00:00<00:08, 1133.61it/s]\u001b[A\n",
      "  9%|▉         | 930/10047 [00:00<00:07, 1140.22it/s]\u001b[A\n",
      " 10%|█         | 1039/10047 [00:00<00:08, 1098.19it/s]\u001b[A\n",
      " 11%|█▏        | 1153/10047 [00:01<00:08, 1107.69it/s]\u001b[A\n",
      " 13%|█▎        | 1275/10047 [00:01<00:07, 1137.45it/s]\u001b[A\n",
      " 14%|█▍        | 1394/10047 [00:01<00:07, 1152.26it/s]\u001b[A\n",
      " 15%|█▌        | 1509/10047 [00:01<00:07, 1108.29it/s]\u001b[A\n",
      " 16%|█▌        | 1620/10047 [00:01<00:08, 1040.24it/s]\u001b[A\n",
      " 17%|█▋        | 1725/10047 [00:01<00:08, 991.27it/s] \u001b[A\n",
      " 18%|█▊        | 1826/10047 [00:01<00:08, 953.41it/s]\u001b[A\n",
      " 19%|█▉        | 1923/10047 [00:01<00:08, 922.86it/s]\u001b[A\n",
      " 20%|██        | 2032/10047 [00:01<00:08, 966.39it/s]\u001b[A\n",
      " 21%|██▏       | 2154/10047 [00:01<00:07, 1025.91it/s]\u001b[A\n",
      " 22%|██▏       | 2259/10047 [00:02<00:07, 992.41it/s] \u001b[A\n",
      " 23%|██▎       | 2360/10047 [00:02<00:08, 950.01it/s]\u001b[A\n",
      " 24%|██▍       | 2457/10047 [00:02<00:08, 926.56it/s]\u001b[A\n",
      " 25%|██▌       | 2551/10047 [00:02<00:08, 927.66it/s]\u001b[A\n",
      " 26%|██▋       | 2645/10047 [00:02<00:08, 917.43it/s]\u001b[A\n",
      " 27%|██▋       | 2738/10047 [00:02<00:08, 912.00it/s]\u001b[A\n",
      " 28%|██▊       | 2835/10047 [00:02<00:07, 927.82it/s]\u001b[A\n",
      " 29%|██▉       | 2933/10047 [00:02<00:07, 941.34it/s]\u001b[A\n",
      " 30%|███       | 3051/10047 [00:02<00:06, 1000.75it/s]\u001b[A\n",
      " 32%|███▏      | 3172/10047 [00:03<00:06, 1054.92it/s]\u001b[A\n",
      " 33%|███▎      | 3288/10047 [00:03<00:06, 1083.44it/s]\u001b[A\n",
      " 34%|███▍      | 3403/10047 [00:03<00:06, 1100.41it/s]\u001b[A\n",
      " 35%|███▍      | 3515/10047 [00:03<00:05, 1098.02it/s]\u001b[A\n",
      " 36%|███▌      | 3627/10047 [00:03<00:05, 1102.92it/s]\u001b[A\n",
      " 37%|███▋      | 3738/10047 [00:03<00:05, 1084.90it/s]\u001b[A\n",
      " 38%|███▊      | 3852/10047 [00:03<00:05, 1100.80it/s]\u001b[A\n",
      " 40%|███▉      | 3971/10047 [00:03<00:05, 1125.31it/s]\u001b[A\n",
      " 41%|████      | 4084/10047 [00:03<00:05, 1111.08it/s]\u001b[A\n",
      " 42%|████▏     | 4207/10047 [00:03<00:05, 1142.57it/s]\u001b[A\n",
      " 43%|████▎     | 4327/10047 [00:04<00:04, 1158.02it/s]\u001b[A\n",
      " 44%|████▍     | 4444/10047 [00:04<00:04, 1157.22it/s]\u001b[A\n",
      " 45%|████▌     | 4560/10047 [00:04<00:04, 1127.47it/s]\u001b[A\n",
      " 47%|████▋     | 4674/10047 [00:04<00:05, 1038.37it/s]\u001b[A\n",
      " 48%|████▊     | 4780/10047 [00:04<00:05, 961.84it/s] \u001b[A\n",
      " 49%|████▊     | 4879/10047 [00:04<00:05, 964.09it/s]\u001b[A\n",
      " 50%|████▉     | 4996/10047 [00:04<00:04, 1015.84it/s]\u001b[A\n",
      " 51%|█████     | 5106/10047 [00:04<00:04, 1039.36it/s]\u001b[A\n",
      " 52%|█████▏    | 5228/10047 [00:04<00:04, 1086.19it/s]\u001b[A\n",
      " 53%|█████▎    | 5346/10047 [00:05<00:04, 1112.17it/s]\u001b[A\n",
      " 54%|█████▍    | 5462/10047 [00:05<00:04, 1124.31it/s]\u001b[A\n",
      " 55%|█████▌    | 5576/10047 [00:05<00:03, 1118.85it/s]\u001b[A\n",
      " 57%|█████▋    | 5689/10047 [00:05<00:03, 1117.14it/s]\u001b[A\n",
      " 58%|█████▊    | 5805/10047 [00:05<00:03, 1128.49it/s]\u001b[A\n",
      " 59%|█████▉    | 5919/10047 [00:05<00:03, 1121.00it/s]\u001b[A\n",
      " 60%|██████    | 6032/10047 [00:05<00:03, 1080.36it/s]\u001b[A\n",
      " 61%|██████    | 6141/10047 [00:05<00:03, 991.69it/s] \u001b[A\n",
      " 62%|██████▏   | 6242/10047 [00:05<00:04, 942.14it/s]\u001b[A\n",
      " 63%|██████▎   | 6357/10047 [00:06<00:03, 995.87it/s]\u001b[A\n",
      " 64%|██████▍   | 6475/10047 [00:06<00:03, 1043.04it/s]\u001b[A\n",
      " 66%|██████▌   | 6597/10047 [00:06<00:03, 1088.47it/s]\u001b[A\n",
      " 67%|██████▋   | 6718/10047 [00:06<00:02, 1121.00it/s]\u001b[A\n",
      " 68%|██████▊   | 6837/10047 [00:06<00:02, 1140.58it/s]\u001b[A\n",
      " 69%|██████▉   | 6953/10047 [00:06<00:02, 1146.33it/s]\u001b[A\n",
      " 70%|███████   | 7069/10047 [00:06<00:02, 1109.92it/s]\u001b[A\n",
      " 71%|███████▏  | 7181/10047 [00:06<00:02, 1077.69it/s]\u001b[A\n",
      " 73%|███████▎  | 7290/10047 [00:06<00:02, 984.39it/s] \u001b[A\n",
      " 74%|███████▎  | 7391/10047 [00:06<00:02, 949.86it/s]\u001b[A\n",
      " 75%|███████▍  | 7488/10047 [00:07<00:02, 928.17it/s]\u001b[A\n",
      " 75%|███████▌  | 7585/10047 [00:07<00:02, 940.05it/s]\u001b[A\n",
      " 76%|███████▋  | 7682/10047 [00:07<00:02, 946.92it/s]\u001b[A\n",
      " 77%|███████▋  | 7784/10047 [00:07<00:02, 966.06it/s]\u001b[A\n",
      " 78%|███████▊  | 7882/10047 [00:07<00:02, 967.33it/s]\u001b[A\n",
      " 79%|███████▉  | 7980/10047 [00:07<00:02, 964.89it/s]\u001b[A\n",
      " 80%|████████  | 8077/10047 [00:07<00:02, 955.36it/s]\u001b[A\n",
      " 81%|████████▏ | 8173/10047 [00:07<00:02, 911.59it/s]\u001b[A\n",
      " 82%|████████▏ | 8266/10047 [00:07<00:01, 914.65it/s]\u001b[A\n",
      " 83%|████████▎ | 8358/10047 [00:08<00:02, 804.20it/s]\u001b[A\n",
      " 84%|████████▍ | 8442/10047 [00:08<00:02, 780.48it/s]\u001b[A\n",
      " 85%|████████▍ | 8523/10047 [00:08<00:01, 775.55it/s]\u001b[A\n",
      " 86%|████████▌ | 8616/10047 [00:08<00:01, 815.14it/s]\u001b[A\n",
      " 87%|████████▋ | 8715/10047 [00:08<00:01, 860.72it/s]\u001b[A\n",
      " 88%|████████▊ | 8811/10047 [00:08<00:01, 887.40it/s]\u001b[A\n",
      " 89%|████████▊ | 8910/10047 [00:08<00:01, 913.82it/s]\u001b[A\n",
      " 90%|████████▉ | 9003/10047 [00:08<00:01, 895.31it/s]\u001b[A\n",
      " 91%|█████████ | 9097/10047 [00:08<00:01, 906.62it/s]\u001b[A\n",
      " 92%|█████████▏| 9195/10047 [00:08<00:00, 927.20it/s]\u001b[A\n",
      " 92%|█████████▏| 9292/10047 [00:09<00:00, 939.55it/s]\u001b[A\n",
      " 93%|█████████▎| 9387/10047 [00:09<00:00, 940.88it/s]\u001b[A\n",
      " 94%|█████████▍| 9488/10047 [00:09<00:00, 958.55it/s]\u001b[A\n",
      " 95%|█████████▌| 9586/10047 [00:09<00:00, 963.55it/s]\u001b[A\n",
      " 96%|█████████▋| 9688/10047 [00:09<00:00, 978.80it/s]\u001b[A\n",
      " 97%|█████████▋| 9787/10047 [00:09<00:00, 975.38it/s]\u001b[A\n",
      " 98%|█████████▊| 9885/10047 [00:09<00:00, 968.75it/s]\u001b[A\n",
      "100%|██████████| 10047/10047 [00:09<00:00, 1010.96it/s]A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ TRUCATION: ------\n",
      "Avg distance overestimated:  1.009478672985782\n",
      "Avg distance underestimated:  0.7558967059780398\n",
      "E_admiss:  0.021001293918582662\n",
      "E_out:  0.6824922862546033\n",
      "Avg distance over Manhattan:  4.326841031348544\n",
      "Avg distance under Manhattan:  1.026266416510319\n",
      "Percent over Manhattan:  0.5365780830098537\n",
      "Percent under Manhattan:  0.31830397133472677\n",
      "\n",
      "\n",
      "------ ROUNDED: ------\n",
      "Avg distance overestimated:  1.0260688\n",
      "Avg distance underestimated:  0.3046875\n",
      "E_admiss:  0.09545137852095152\n",
      "E_out:  0.3494575495172688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dist_over_i = []\n",
    "misclass_i = 0\n",
    "dist_under_i = []\n",
    "dist_over_man_i = []\n",
    "dist_under_man_i = []\n",
    "\n",
    "dist_over_r = []\n",
    "misclass_r = 0\n",
    "dist_under_r = []\n",
    "\n",
    "model = luka_model(X_test_2, Y_test)\n",
    "\n",
    "for i in tqdm(range(len(X_test))):\n",
    "    nn_heur_i = int(model.predict(X_test_2[i:(i+1),:]))\n",
    "    nn_heur_r = np.around(model.predict(X_test_2[i:(i+1),:]))\n",
    "    man_heur = h.manhattan(X_test[i].reshape(4,4), model)\n",
    "    y = Y_test[i]\n",
    "    \n",
    "    ### TRUNCATE ###\n",
    "    if (nn_heur_i > y):\n",
    "        dist_over_i.append(nn_heur_i - y)\n",
    "    \n",
    "    if (nn_heur_i <= y):\n",
    "        dist_under_i.append(y - nn_heur_i)\n",
    "    \n",
    "    if (nn_heur_i != y):\n",
    "        misclass_i += 1\n",
    "    \n",
    "    if (nn_heur_i > man_heur):\n",
    "        dist_over_man_i.append(nn_heur_i - man_heur)\n",
    "        \n",
    "    if (nn_heur_i < man_heur):\n",
    "        dist_under_man_i.append(man_heur - nn_heur_i)\n",
    "    \n",
    "        \n",
    "    ##### ROUND ##### \n",
    "    if (nn_heur_r > y):\n",
    "        dist_over_r.append(nn_heur_r - y)\n",
    "    \n",
    "    if (nn_heur_r <= y):\n",
    "        dist_under_r.append(y - nn_heur_r)\n",
    "    \n",
    "    if (nn_heur_r != y):\n",
    "        misclass_r += 1\n",
    "    \n",
    "avg_dist_over_i = np.mean(np.asarray(dist_over_i))\n",
    "avg_dist_under_i = np.mean(np.asarray(dist_under_i))\n",
    "out_sample_error_i = misclass_i / len(X_test)\n",
    "avg_dist_over_man_i = np.mean(np.asarray(dist_over_man_i))\n",
    "avg_dist_under_man_i = np.mean(np.asarray(dist_under_man_i))\n",
    "\n",
    "avg_dist_over_r = np.mean(np.asarray(dist_over_r))\n",
    "avg_dist_under_r = np.mean(np.asarray(dist_under_r))\n",
    "out_sample_error_r = misclass_r / len(X_test)\n",
    " \n",
    "print(\"------ TRUCATION: ------\")\n",
    "print(\"Avg distance overestimated: \", avg_dist_over_i)\n",
    "print(\"Avg distance underestimated: \", avg_dist_under_i)\n",
    "print(\"E_admiss: \", len(dist_over_i)/len(X_test))\n",
    "print(\"E_out: \", out_sample_error_i)\n",
    "print(\"Avg distance over Manhattan: \", avg_dist_over_man_i)\n",
    "print(\"Avg distance under Manhattan: \", avg_dist_under_man_i)\n",
    "print(\"Percent over Manhattan: \", len(dist_over_man_i)/len(X_test))\n",
    "print(\"Percent under Manhattan: \", len(dist_under_man_i)/len(X_test))\n",
    "print(\"\\n\")\n",
    "print(\"------ ROUNDED: ------\")\n",
    "print(\"Avg distance overestimated: \", avg_dist_over_r)\n",
    "print(\"Avg distance underestimated: \", avg_dist_under_r)\n",
    "print(\"E_admiss: \", len(dist_over_r)/len(X_test))\n",
    "print(\"E_out: \", out_sample_error_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"one_HAWT_356_17_nn.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# DON\\'T HAVE TO RUN THIS CELL AGAIN, only to transform Yasmin_5_16_40360.txt.\\n\\n(X,Y) = load_data(\\'Uncombined Data Files/Yasmin_5_16_40360.txt\\')\\nknn_model_all = NearestNeighbors(n_neighbors=50, n_jobs = -1).fit(X,Y)\\nX_2 = gen_features(X, X, knn_model_all)\\nX_Y = np.column_stack((X_2, Y))\\nnp.savetxt(\"Yasmin_40360_50knn_Trans.csv\", X_Y, delimiter=\\',\\')\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# DON'T HAVE TO RUN THIS CELL AGAIN, only to transform Yasmin_5_16_40360.txt.\n",
    "\n",
    "(X,Y) = load_data('Uncombined Data Files/Yasmin_5_16_40360.txt')\n",
    "knn_model_all = NearestNeighbors(n_neighbors=50, n_jobs = -1).fit(X,Y)\n",
    "X_2 = gen_features(X, X, knn_model_all)\n",
    "X_Y = np.column_stack((X_2, Y))\n",
    "np.savetxt(\"Yasmin_40360_50knn_Trans.csv\", X_Y, delimiter=',')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef heur_boi(board, model):\\n    \"\"\"\\n    This function takes in a board and a trained NN model and returns\\n    the heuristic the model predicts.\\n    \"\"\"\\n    return 0\\n    #[[pred]] = model.predict(board)\\n    #return round(pred)\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def string_to_test_info(string):\n",
    "    \"\"\"\n",
    "    given a string containing the standard form of test info, returns tuple of \n",
    "    board, number of states to solution, time, and lenght of solution\n",
    "    \"\"\"\n",
    "    split = string.split(\"!\")\n",
    "    board = io.string_to_board(split[0])\n",
    "    n_states = int(split[1])\n",
    "    time = float(split[2])\n",
    "    sol_len = int(split[3])\n",
    "    return (board, n_states, time, sol_len)\n",
    "\n",
    "\n",
    "def load_boards(filename):\n",
    "    \"\"\"\n",
    "    given name of file containing test boards, loads all test boards\n",
    "    \"\"\"\n",
    "    file = open(filename, \"r\")\n",
    "\n",
    "    boards = []\n",
    "    n_states = []\n",
    "    times = []\n",
    "    dists = []\n",
    "\n",
    "    for line in file:\n",
    "        (board, c_states, c_time, sol_len) = string_to_test_info(line)\n",
    "        boards.append(board)\n",
    "        n_states.append(c_states)\n",
    "        times.append(c_time)\n",
    "        dists.append(sol_len)\n",
    "\n",
    "    return (boards, n_states, times, dists)\n",
    "\n",
    "def run_testing(data_file, model, h_func):\n",
    "    \"\"\"\n",
    "    given a data_file containing testing data, a model, and heuristic function\n",
    "    for said model, computes average number of states to solution, number to \n",
    "    times solution length is non-optimal, and average estimates of solution \n",
    "    lengths\n",
    "    \"\"\"\n",
    "    (boards, n_states, times, dists) = load_boards(data_file)\n",
    "\n",
    "    cust_states = []\n",
    "    cust_wrong = 0\n",
    "    cust_distance = []\n",
    "\n",
    "    for i in tqdm(range(len(boards))):\n",
    "        #(c_states, c_time, sol_path) = s.solve(boards[i], h_func, model)\n",
    "        (c_states, c_time, sol_path) = s.solve(boards[i], h_func, model)\n",
    "        cust_states.append(c_states)\n",
    "        sol_len = len(sol_path) - 1\n",
    "        if not (sol_len == dists[i]):\n",
    "            cust_wrong += 1\n",
    "        cust_distance.append(sol_len)\n",
    "\n",
    "    print(\"average number of states explored to find solution:\")\n",
    "    print(\"\\tfor learned model: \" + str(np.mean(cust_states)))\n",
    "    print(\"\\tfor manhattan distance: \" + str(np.mean(n_states)))\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\"solution was non-optimal \" + str(cust_wrong / NUM_TEST_BOARDS * 100) + \"% of the time\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\"average length of solution path was:\")\n",
    "    print(\"\\tfor learned model: \" + str(np.mean(cust_distance)))\n",
    "    print(\"\\tfor manhattan distance: \" + str(np.mean(dists)))\n",
    "'''\n",
    "def heur_boi(board, model):\n",
    "    \"\"\"\n",
    "    This function takes in a board and a trained NN model and returns\n",
    "    the heuristic the model predicts.\n",
    "    \"\"\"\n",
    "    return 0\n",
    "    #[[pred]] = model.predict(board)\n",
    "    #return round(pred)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "def heur_boi(board, model):\n",
    "    b1 = board.reshape(16)\n",
    "    man = np.array(h.manhattan(b1.reshape(4,4), None))\n",
    "    ham = np.array(h.hamming(b1.reshape(4,4), None))\n",
    "    \n",
    "    b1_str = np.array_str(b1)\n",
    "    #b2 = np.concatenate((b1, man, ham), axis=None)\n",
    "    #b2_str = np.array_str(b2)\n",
    "    \n",
    "    if b1_str in dic:\n",
    "        return dic.get(b1_str)\n",
    "    else:\n",
    "        # transform board\n",
    "        X_2 = gen_features(np.asarray([b1]), X_train, knn_model)\n",
    "        [[pred]] = model.predict(X_2[0].reshape(1,-1))\n",
    "        dic[b1_str] = int(pred)\n",
    "        return dic[b1_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 1/15 [00:06<01:31,  6.51s/it]\u001b[A\n",
      " 13%|█▎        | 2/15 [00:10<01:13,  5.69s/it]\u001b[A\n",
      " 20%|██        | 3/15 [01:33<05:48, 29.02s/it]\u001b[A\n",
      " 27%|██▋       | 4/15 [02:09<05:40, 30.91s/it]\u001b[A\n",
      " 33%|███▎      | 5/15 [04:18<10:05, 60.56s/it]\u001b[A\n",
      " 40%|████      | 6/15 [09:40<20:49, 138.85s/it]\u001b[A\n",
      " 47%|████▋     | 7/15 [09:58<13:40, 102.62s/it]\u001b[A\n",
      " 53%|█████▎    | 8/15 [10:32<09:33, 81.97s/it] \u001b[A\n",
      " 60%|██████    | 9/15 [11:40<07:47, 77.91s/it]\u001b[A\n",
      " 67%|██████▋   | 10/15 [12:09<05:16, 63.29s/it]\u001b[A\n",
      " 73%|███████▎  | 11/15 [56:41<56:23, 845.77s/it]\u001b[A\n",
      " 80%|████████  | 12/15 [57:23<30:14, 604.73s/it]\u001b[A\n",
      " 87%|████████▋ | 13/15 [57:51<14:23, 431.73s/it]\u001b[A\n",
      " 93%|█████████▎| 14/15 [59:38<05:34, 334.37s/it]\u001b[A\n",
      "100%|██████████| 15/15 [1:01:12<00:00, 244.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average number of states explored to find solution:\n",
      "\tfor learned model: 1112.6\n",
      "\tfor manhattan distance: 29145.266666666666\n",
      "----------------------------------------------------\n",
      "solution was non-optimal 0.2% of the time\n",
      "----------------------------------------------------\n",
      "average length of solution path was:\n",
      "\tfor learned model: 27.466666666666665\n",
      "\tfor manhattan distance: 27.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "run_testing('baby_test.txt', model, heur_boi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(np.zeros((1,82)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(np.arange(82).T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arange(82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray([np.arange(82)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = gen_features(np.asarray([np.arange(16)]), X_train, knn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = gen_features(np.asarray([np.arange(16)]), X_train, knn_model)\n",
    "[[pred]] = model.predict(t3[0].reshape(1,-1))\n",
    "print(round(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([2,6,3,4,12,5,10,15,1,14,16,8,13,7,9,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = gen_features(np.asarray([np.array([5,7,16,2,6,14,12,1,9,3,11,15,13,10,8,4])]), X_train, knn_model)\n",
    "[[pred]] = model.predict(t3[0].reshape(1,-1))\n",
    "print(round(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.calc_displacements(np.array([5,7,16,2,6,14,12,1,9,3,11,15,13,10,8,4]).reshape(4,4)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = np.zeros()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
