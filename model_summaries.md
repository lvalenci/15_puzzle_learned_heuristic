# Model Summaries
- exp_loss_2: architecture is 256 input layer, 16 unit dense layer with ReLu activiation, single output unit with linear activation. Trained for 10 epochs with Adam as the optimizer and using exp_loss (loss = K.exp((y_pred - y_true)) / 10;loss = loss + K.square((y_pred - y_true) / 2);loss = K.mean(loss, axis = 1)). Resulted in prediction being less than manhattan distance 6.88% of the time and greater than actual distance 17.67% of the time.
- nn_exp_loss_2b: architecture is 256 input layer, 256 unit dense layer with ReLu activiation, single output unit with linear activation. Trained for 10 epochs with Adam as the optimizer and using exp_loss_2 (loss = K.exp((y_pred - y_true)) / 2;loss = loss + K.square(y_pred - y_true);loss = K.mean(loss, axis = 1)). Resulted in prediction less than manhattan distance 3.688% of the time and greater than acutal distance 14.467% of the time
- nn_exp_loss_2c: architecture is 256 input layer, 256 unit dense layer with ReLu activiation, 16 unit dense layer with ReLu activation, single output unit with linear activation. Trained for 15 epochs with Adam as the optimizer and using exp_loss_2 (loss = K.exp((y_pred - y_true)) / 2;loss = loss + K.square(y_pred - y_true);loss = K.mean(loss, axis = 1)). Resulted in prediction less than manhattan distance 5.3% of time and greater than actual distance 13% of the time
- nn_exp_loss_2d: architecture is 256 input layer, 256 unit dense layer with ReLu activiation, 0.1 dropout layer, 64 unit dense layer with ReLu activation, 0.1 dropout layer, 8 unit dense layer with ReLu activation, and single output unit with linear activation. Trained for 15 epochs with Adam as the optimizer and using exp_loss_2 (loss = K.exp((y_pred - y_true)) / 2;loss = loss + K.square(y_pred - y_true);loss = K.mean(loss, axis = 1)). Resulted in prediction less than manhattan distance 5.56% of time and greater than actual distance 10.92% of the time
- nn_shift_mse: architecture is 256 input layer, 256 unit dense layer with ReLu activiation, 16 unit dense layer with ReLu activation, single output unit with linear activation. Trained for 15 epochs with Adam as the optimizer and using shift_mse loss ((1 + 1/ (1 + K.exp(-(y_pred - y_true)))))))) * K.square(y_pred - y_true); loss = K.mean(loss, axis = 1)). Resulted in prediction less than manhattan distance 2.76% of time and greater than actual distance 13.53% of the time
- nn_shift_mse2: architecture is 256 input layer, 256 unit dense layer with ReLu activiation, 256 unit dense layer with ReLu activation, single output unit with linear activation. Trained for 15 epochs with Adam as the optimizer and using shift_mse loss ((1 + 1/ (1 + K.exp(-(y_pred - y_true)))))))) * K.square(y_pred - y_true); loss = K.mean(loss, axis = 1)). Resulted in prediction less than manhattan distance 1.88 of time and greater than actual distance 21.01 of the time
- nn_shift_mse3: architecture is 256 input layer, 256 unit dense layer with ReLu activiation, 0.1 dropout layer, 64 unit dense layer with ReLu activation, 0.1 dropout layer, 8 unit dense layer with ReLu activation, and single output unit with linear activation. Trained for 15 epochs with Adam as the optimizer and using shift_mse loss ((1 + 1/ (1 + K.exp(-(y_pred - y_true)))))))) * K.square(y_pred - y_true); loss = K.mean(loss, axis = 1)). Resulted in prediction less than manhattan distance 5.67% of time and greater than actual distance 11.59% of the time
- neural_net_basic_mse: architecture is 256 input layer, 256 unit dense layer with ReLu activiation, 0.1 dropout layer, 64 unit dense layer with ReLu activation, 0.1 dropout layer, 8 unit dense layer with ReLu activation, and single output unit with linear activation. Trained for 15 epochs with Adam as the optimizer and using mse loss. Resulted in prediction less than manhattan distance 2.01% of time and greater than actual distance 21.17% of the time